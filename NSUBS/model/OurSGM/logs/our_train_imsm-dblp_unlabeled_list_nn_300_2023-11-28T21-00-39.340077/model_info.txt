main_func                  : here
filter                     : DPiso
order                      : GQL
engine                     : LFTJ
model                      : our
do_train                   : True
do_validation              : False
do_test                    : False
num_cs_refinements         : 3
search_constraints         : isomorphic
thresh_throwaway_qsa       : 1e-08
reward_method              : exp_depth
reward_method_exp_coeff    : 100
reward_method_normalize    : True
debug_embeddings_every_k_iters : None
loss_type                  : mse_bounded
regularization             : 0.05
add_gnd_truth              : add_gnd_truth_at_start
mark_best_as_true          : True
eps_decay_config           : {'start_eps': 0.0, 'end_eps': 0.0, 'num_iters': 5}
train_dataset              : imsm-dblp_unlabeled
val_dataset                : imsm-dblp_unlabeled
train_sample_size_li       : [None]
val_sample_size_li         : [None]
train_subgroup             : list
train_subgroup_list        : ['dense_64']
train_path_indices         : [[0, 5], [0, 5], [0, 5], [0, 5], [0, 50], [0, 50], [0, 50]]
val_subgroup               : dense_64
val_subgroup_list          : None
val_path_indices           : [95, 100]
pretrain                   : False
imitation                  : False
show_precision             : False
buffer_size                : 128
batch_size                 : 1
print_buffer_stats         : True
prune_trivial_rewards      : False
print_rpreds_stats         : True
glsearch                   : False
num_epochs_per_learn       : 128
num_outer_epoch            : 64
skip_data_leakage          : False
load_model                 : None
append_ldf                 : True
val_every_games            : 1
save_every                 : 1000
matching_order             : nn
use_is_early_pruning       : False
MCTS_train                 : True
MCTS_test                  : False
MCTS_printQU               : True
MCTS_num_iters_max         : 120
MCTS_num_iters_per_action  : 10.0
MCTS_temp                  : 5.0
MCTS_temp_inner            : 1.0
MCTS_cpuct                 : 10.0
MCTS_eps_in_U              : 1e-08
MCTS_backup_to_real_root   : True
d_enc                      : 128
encoder_type               : mlp
use_NN_for_u_score         : False
dvn_config                 : {'preencoder': {'type': 'concat+mlp'}, 'encoder': {'type': 'GNNConsensusEncoder', 'gnn_type': 'OurGMNv2', 'gnn_subtype': 'graphgps', 'hidden_gnn_dims': [128, 128, 128, 128], 'shared_gnn_weights': False, 'shared_encoder': False, 'q2t': True, 't2q': True, 'consensus_cfg_li': None}, 'decoder_dvn': {'type': 'Query', 'simple_decoder': {'mlp_att': [128, 8, 1], 'mlp_val': [128, 128], 'mlp_final': [128, 32, 16, 8, 4, 1]}}, 'decoder_policy': {'type': 'bilinear_custom', 'similarity_decoder': {'mlp_in_dims': [128, 128], 'mlp_out_dims': [256, 32, 16, 8, 1], 'g_emb': 128}}}
cache_embeddings           : True
k_sample_cross_graph       : None
regret_iters_train         : 1
regret_iters_test          : 1
use_node_mask_diameter     : False
num_iters_threshold        : -1
timeout                    : 300
time                       : 2023-10-20_11-44-50
time_analysis              : False
learning_timeout           : 120
timeout_val                : 120
num_iters_threshold_val    : 200
plot_tree                  : False
plot_solution              : False
plot_logs                  : False
save_search                : False
device                     : cuda:3
encoder_structure          : encoder2
graphgps_config_path       : email_GateGCN_RSWE.yaml
lr                         : 0.001
fix_randomness             : True
random_seed                : 123
skip_if_action_space_less_than : None
apply_norm                 : True
ckpt                       : 
user                       : kli16
hostname                   : vector.cs.gsu.edu
ts                         : 2023-11-28T21-00-39.340077

python /home/kli16/ISM_custom/esm_NSUBS_RWSE_trans/esm/NSUBS/model/OurSGM/main.py --MCTS_backup_to_real_root=True  --MCTS_cpuct=10.0  --MCTS_eps_in_U=1e-08  --MCTS_num_iters_max=120  --MCTS_num_iters_per_action=10.0  --MCTS_printQU=True  --MCTS_temp=5.0  --MCTS_temp_inner=1.0  --MCTS_test=False  --MCTS_train=True  --add_gnd_truth=add_gnd_truth_at_start  --append_ldf=True  --apply_norm=True  --batch_size=1  --buffer_size=128  --cache_embeddings=True  --ckpt=  --d_enc=128  --debug_embeddings_every_k_iters=None  --device=cuda:3  --do_test=False  --do_train=True  --do_validation=False  --dvn_config={'preencoder': {'type': 'concat+mlp'}, 'encoder': {'type': 'GNNConsensusEncoder', 'gnn_type': 'OurGMNv2', 'gnn_subtype': 'graphgps', 'hidden_gnn_dims': [128, 128, 128, 128], 'shared_gnn_weights': False, 'shared_encoder': False, 'q2t': True, 't2q': True, 'consensus_cfg_li': None}, 'decoder_dvn': {'type': 'Query', 'simple_decoder': {'mlp_att': [128, 8, 1], 'mlp_val': [128, 128], 'mlp_final': [128, 32, 16, 8, 4, 1]}}, 'decoder_policy': {'type': 'bilinear_custom', 'similarity_decoder': {'mlp_in_dims': [128, 128], 'mlp_out_dims': [256, 32, 16, 8, 1], 'g_emb': 128}}}  --encoder_structure=encoder2  --encoder_type=mlp  --engine=LFTJ  --eps_decay_config={'start_eps': 0.0, 'end_eps': 0.0, 'num_iters': 5}  --filter=DPiso  --fix_randomness=True  --glsearch=False  --graphgps_config_path=email_GateGCN_RSWE.yaml  --hostname=vector.cs.gsu.edu  --imitation=False  --k_sample_cross_graph=None  --learning_timeout=120  --load_model=None  --loss_type=mse_bounded  --lr=0.001  --main_func=here  --mark_best_as_true=True  --matching_order=nn  --model=our  --num_cs_refinements=3  --num_epochs_per_learn=128  --num_iters_threshold=-1  --num_iters_threshold_val=200  --num_outer_epoch=64  --order=GQL  --plot_logs=False  --plot_solution=False  --plot_tree=False  --pretrain=False  --print_buffer_stats=True  --print_rpreds_stats=True  --prune_trivial_rewards=False  --random_seed=123  --regret_iters_test=1  --regret_iters_train=1  --regularization=0.05  --reward_method=exp_depth  --reward_method_exp_coeff=100  --reward_method_normalize=True  --save_every=1000  --save_search=False  --search_constraints=isomorphic  --show_precision=False  --skip_data_leakage=False  --skip_if_action_space_less_than=None  --thresh_throwaway_qsa=1e-08  --time=2023-10-20_11-44-50  --time_analysis=False  --timeout=300  --timeout_val=120  --train_dataset=imsm-dblp_unlabeled  --train_path_indices=[[0, 5], [0, 5], [0, 5], [0, 5], [0, 50], [0, 50], [0, 50]]  --train_sample_size_li=[None]  --train_subgroup=list  --train_subgroup_list=['dense_64']  --use_NN_for_u_score=False  --use_is_early_pruning=False  --use_node_mask_diameter=False  --user=kli16  --val_dataset=imsm-dblp_unlabeled  --val_every_games=1  --val_path_indices=[95, 100]  --val_sample_size_li=[None]  --val_subgroup=dense_64  --val_subgroup_list=None

model:
DVN_wrapper(
  (dvn): DVN(
    (pre_encoder): PreEncoderConcatSelectedOneHotAndMLP(
      (mlp_q): Linear(in_features=49, out_features=64, bias=True)
      (mlp_t): Linear(in_features=49, out_features=64, bias=True)
      (mlp_RWSEq): Linear(in_features=20, out_features=64, bias=True)
      (mlp_RWSEt): Linear(in_features=20, out_features=64, bias=True)
    )
    (encoder): GNNConsensusEncoder(
      (gnn_wrapper_li): ModuleList(
        (0-3): 4 x GNNWrapper(
          (gnnm): OurGMNCustomWrapper(
            (gmn_inter): OurGMNCustomInter(
              (mlp_att_cross_q): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_att_cross_t): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_val_cross_q): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_val_cross_t): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_merge_q): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=256, out_features=128, bias=True)
                )
              )
              (mlp_merge_t): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=384, out_features=128, bias=True)
                )
              )
            )
            (gmn_intra): OurGMNCustomIntra(
              (gnn_t): GPSLayer(
                summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
                (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
                (act_fn_ff): ReLU()
                (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
              (gnn_q): GPSLayer(
                summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
                (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
                (act_fn_ff): ReLU()
                (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (jk): JumpingKnowledge(max)
    )
    (gt_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 128)
    )
    (gq_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 128)
    )
    (decoder_policy): BilinearDecoder(
      (encoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (decoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=384, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=1, bias=True)
        )
      )
    )
    (decoder_value): QueryDecoder(
      (mlp_att): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=1, bias=True)
        )
      )
      (mlp_val): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_final): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=4, bias=True)
          (4): Linear(in_features=4, out_features=1, bias=True)
        )
      )
      (norm): NormalizeAttention()
    )
    (norm_li): ModuleList(
      (0-3): 4 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (mlp_final): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): Linear(in_features=16, out_features=8, bias=True)
      (3): Linear(in_features=8, out_features=4, bias=True)
      (4): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)


Details:
DVN_wrapper(
  (dvn): DVN(
    (pre_encoder): PreEncoderConcatSelectedOneHotAndMLP(
      (mlp_q): Linear(in_features=49, out_features=64, bias=True)
      (mlp_t): Linear(in_features=49, out_features=64, bias=True)
      (mlp_RWSEq): Linear(in_features=20, out_features=64, bias=True)
      (mlp_RWSEt): Linear(in_features=20, out_features=64, bias=True)
    )
    (encoder): GNNConsensusEncoder(
      (gnn_wrapper_li): ModuleList(
        (0-3): 4 x GNNWrapper(
          (gnnm): OurGMNCustomWrapper(
            (gmn_inter): OurGMNCustomInter(
              (mlp_att_cross_q): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_att_cross_t): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_val_cross_q): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_val_cross_t): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=128, out_features=128, bias=True)
                )
              )
              (mlp_merge_q): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=256, out_features=128, bias=True)
                )
              )
              (mlp_merge_t): MLP(
                (activation): ELU(alpha=1.0)
                (layers): ModuleList(
                  (0): Linear(in_features=384, out_features=128, bias=True)
                )
              )
            )
            (gmn_intra): OurGMNCustomIntra(
              (gnn_t): GPSLayer(
                summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
                (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
                (act_fn_ff): ReLU()
                (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
              (gnn_q): GPSLayer(
                summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
                (local_model): GatedGCNLayer()
                (self_attn): MultiheadAttention(
                  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
                )
                (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (dropout_local): Dropout(p=0.0, inplace=False)
                (dropout_attn): Dropout(p=0.0, inplace=False)
                (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
                (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
                (act_fn_ff): ReLU()
                (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
                (ff_dropout1): Dropout(p=0.0, inplace=False)
                (ff_dropout2): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (jk): JumpingKnowledge(max)
    )
    (gt_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 128)
    )
    (gq_egde_encoder): DummyEdgeEncoder(
      (encoder): Embedding(1, 128)
    )
    (decoder_policy): BilinearDecoder(
      (encoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (decoder): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=384, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=1, bias=True)
        )
      )
    )
    (decoder_value): QueryDecoder(
      (mlp_att): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=8, bias=True)
          (1): Linear(in_features=8, out_features=1, bias=True)
        )
      )
      (mlp_val): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_final): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=32, bias=True)
          (1): Linear(in_features=32, out_features=16, bias=True)
          (2): Linear(in_features=16, out_features=8, bias=True)
          (3): Linear(in_features=8, out_features=4, bias=True)
          (4): Linear(in_features=4, out_features=1, bias=True)
        )
      )
      (norm): NormalizeAttention()
    )
    (norm_li): ModuleList(
      (0-3): 4 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (mlp_final): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=64, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=16, bias=True)
      (2): Linear(in_features=16, out_features=8, bias=True)
      (3): Linear(in_features=8, out_features=4, bias=True)
      (4): Linear(in_features=4, out_features=1, bias=True)
    )
  )
)

dvn
DVN(
  (pre_encoder): PreEncoderConcatSelectedOneHotAndMLP(
    (mlp_q): Linear(in_features=49, out_features=64, bias=True)
    (mlp_t): Linear(in_features=49, out_features=64, bias=True)
    (mlp_RWSEq): Linear(in_features=20, out_features=64, bias=True)
    (mlp_RWSEt): Linear(in_features=20, out_features=64, bias=True)
  )
  (encoder): GNNConsensusEncoder(
    (gnn_wrapper_li): ModuleList(
      (0-3): 4 x GNNWrapper(
        (gnnm): OurGMNCustomWrapper(
          (gmn_inter): OurGMNCustomInter(
            (mlp_att_cross_q): MLP(
              (activation): ELU(alpha=1.0)
              (layers): ModuleList(
                (0): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (mlp_att_cross_t): MLP(
              (activation): ELU(alpha=1.0)
              (layers): ModuleList(
                (0): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (mlp_val_cross_q): MLP(
              (activation): ELU(alpha=1.0)
              (layers): ModuleList(
                (0): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (mlp_val_cross_t): MLP(
              (activation): ELU(alpha=1.0)
              (layers): ModuleList(
                (0): Linear(in_features=128, out_features=128, bias=True)
              )
            )
            (mlp_merge_q): MLP(
              (activation): ELU(alpha=1.0)
              (layers): ModuleList(
                (0): Linear(in_features=256, out_features=128, bias=True)
              )
            )
            (mlp_merge_t): MLP(
              (activation): ELU(alpha=1.0)
              (layers): ModuleList(
                (0): Linear(in_features=384, out_features=128, bias=True)
              )
            )
          )
          (gmn_intra): OurGMNCustomIntra(
            (gnn_t): GPSLayer(
              summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
              (local_model): GatedGCNLayer()
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (dropout_local): Dropout(p=0.0, inplace=False)
              (dropout_attn): Dropout(p=0.0, inplace=False)
              (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
              (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
              (act_fn_ff): ReLU()
              (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (ff_dropout1): Dropout(p=0.0, inplace=False)
              (ff_dropout2): Dropout(p=0.0, inplace=False)
            )
            (gnn_q): GPSLayer(
              summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
              (local_model): GatedGCNLayer()
              (self_attn): MultiheadAttention(
                (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
              )
              (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (dropout_local): Dropout(p=0.0, inplace=False)
              (dropout_attn): Dropout(p=0.0, inplace=False)
              (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
              (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
              (act_fn_ff): ReLU()
              (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (ff_dropout1): Dropout(p=0.0, inplace=False)
              (ff_dropout2): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
    )
    (jk): JumpingKnowledge(max)
  )
  (gt_egde_encoder): DummyEdgeEncoder(
    (encoder): Embedding(1, 128)
  )
  (gq_egde_encoder): DummyEdgeEncoder(
    (encoder): Embedding(1, 128)
  )
  (decoder_policy): BilinearDecoder(
    (encoder): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (decoder): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=384, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=1, bias=True)
      )
    )
  )
  (decoder_value): QueryDecoder(
    (mlp_att): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=8, bias=True)
        (1): Linear(in_features=8, out_features=1, bias=True)
      )
    )
    (mlp_val): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_final): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=32, bias=True)
        (1): Linear(in_features=32, out_features=16, bias=True)
        (2): Linear(in_features=16, out_features=8, bias=True)
        (3): Linear(in_features=8, out_features=4, bias=True)
        (4): Linear(in_features=4, out_features=1, bias=True)
      )
    )
    (norm): NormalizeAttention()
  )
  (norm_li): ModuleList(
    (0-3): 4 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
)

dvn.pre_encoder
PreEncoderConcatSelectedOneHotAndMLP(
  (mlp_q): Linear(in_features=49, out_features=64, bias=True)
  (mlp_t): Linear(in_features=49, out_features=64, bias=True)
  (mlp_RWSEq): Linear(in_features=20, out_features=64, bias=True)
  (mlp_RWSEt): Linear(in_features=20, out_features=64, bias=True)
)

dvn.pre_encoder.mlp_q
Linear(in_features=49, out_features=64, bias=True)

dvn.pre_encoder.mlp_t
Linear(in_features=49, out_features=64, bias=True)

dvn.pre_encoder.mlp_RWSEq
Linear(in_features=20, out_features=64, bias=True)

dvn.pre_encoder.mlp_RWSEt
Linear(in_features=20, out_features=64, bias=True)

dvn.encoder
GNNConsensusEncoder(
  (gnn_wrapper_li): ModuleList(
    (0-3): 4 x GNNWrapper(
      (gnnm): OurGMNCustomWrapper(
        (gmn_inter): OurGMNCustomInter(
          (mlp_att_cross_q): MLP(
            (activation): ELU(alpha=1.0)
            (layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
            )
          )
          (mlp_att_cross_t): MLP(
            (activation): ELU(alpha=1.0)
            (layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
            )
          )
          (mlp_val_cross_q): MLP(
            (activation): ELU(alpha=1.0)
            (layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
            )
          )
          (mlp_val_cross_t): MLP(
            (activation): ELU(alpha=1.0)
            (layers): ModuleList(
              (0): Linear(in_features=128, out_features=128, bias=True)
            )
          )
          (mlp_merge_q): MLP(
            (activation): ELU(alpha=1.0)
            (layers): ModuleList(
              (0): Linear(in_features=256, out_features=128, bias=True)
            )
          )
          (mlp_merge_t): MLP(
            (activation): ELU(alpha=1.0)
            (layers): ModuleList(
              (0): Linear(in_features=384, out_features=128, bias=True)
            )
          )
        )
        (gmn_intra): OurGMNCustomIntra(
          (gnn_t): GPSLayer(
            summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
            (local_model): GatedGCNLayer()
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
            )
            (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (dropout_local): Dropout(p=0.0, inplace=False)
            (dropout_attn): Dropout(p=0.0, inplace=False)
            (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
            (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
            (act_fn_ff): ReLU()
            (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (ff_dropout1): Dropout(p=0.0, inplace=False)
            (ff_dropout2): Dropout(p=0.0, inplace=False)
          )
          (gnn_q): GPSLayer(
            summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
            (local_model): GatedGCNLayer()
            (self_attn): MultiheadAttention(
              (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
            )
            (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (dropout_local): Dropout(p=0.0, inplace=False)
            (dropout_attn): Dropout(p=0.0, inplace=False)
            (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
            (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
            (act_fn_ff): ReLU()
            (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (ff_dropout1): Dropout(p=0.0, inplace=False)
            (ff_dropout2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (jk): JumpingKnowledge(max)
)

dvn.encoder.gnn_wrapper_li
ModuleList(
  (0-3): 4 x GNNWrapper(
    (gnnm): OurGMNCustomWrapper(
      (gmn_inter): OurGMNCustomInter(
        (mlp_att_cross_q): MLP(
          (activation): ELU(alpha=1.0)
          (layers): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (mlp_att_cross_t): MLP(
          (activation): ELU(alpha=1.0)
          (layers): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (mlp_val_cross_q): MLP(
          (activation): ELU(alpha=1.0)
          (layers): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (mlp_val_cross_t): MLP(
          (activation): ELU(alpha=1.0)
          (layers): ModuleList(
            (0): Linear(in_features=128, out_features=128, bias=True)
          )
        )
        (mlp_merge_q): MLP(
          (activation): ELU(alpha=1.0)
          (layers): ModuleList(
            (0): Linear(in_features=256, out_features=128, bias=True)
          )
        )
        (mlp_merge_t): MLP(
          (activation): ELU(alpha=1.0)
          (layers): ModuleList(
            (0): Linear(in_features=384, out_features=128, bias=True)
          )
        )
      )
      (gmn_intra): OurGMNCustomIntra(
        (gnn_t): GPSLayer(
          summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
          (local_model): GatedGCNLayer()
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout_local): Dropout(p=0.0, inplace=False)
          (dropout_attn): Dropout(p=0.0, inplace=False)
          (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
          (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
          (act_fn_ff): ReLU()
          (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (ff_dropout1): Dropout(p=0.0, inplace=False)
          (ff_dropout2): Dropout(p=0.0, inplace=False)
        )
        (gnn_q): GPSLayer(
          summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
          (local_model): GatedGCNLayer()
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (dropout_local): Dropout(p=0.0, inplace=False)
          (dropout_attn): Dropout(p=0.0, inplace=False)
          (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
          (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
          (act_fn_ff): ReLU()
          (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (ff_dropout1): Dropout(p=0.0, inplace=False)
          (ff_dropout2): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.0
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (mlp_att_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_att_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_merge_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (mlp_merge_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=384, out_features=128, bias=True)
        )
      )
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (mlp_att_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_att_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_merge_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
      )
    )
    (mlp_merge_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=384, out_features=128, bias=True)
      )
    )
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter
OurGMNCustomInter(
  (mlp_att_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_att_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_merge_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (mlp_merge_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=384, out_features=128, bias=True)
    )
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_att_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_val_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_q.layers
ModuleList(
  (0): Linear(in_features=256, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_q.layers.0
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=384, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_t.layers
ModuleList(
  (0): Linear(in_features=384, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_inter.mlp_merge_t.layers.0
Linear(in_features=384, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.0.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (mlp_att_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_att_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_merge_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (mlp_merge_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=384, out_features=128, bias=True)
        )
      )
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (mlp_att_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_att_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_merge_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
      )
    )
    (mlp_merge_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=384, out_features=128, bias=True)
      )
    )
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter
OurGMNCustomInter(
  (mlp_att_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_att_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_merge_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (mlp_merge_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=384, out_features=128, bias=True)
    )
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_att_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_val_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_q.layers
ModuleList(
  (0): Linear(in_features=256, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_q.layers.0
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=384, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_t.layers
ModuleList(
  (0): Linear(in_features=384, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_inter.mlp_merge_t.layers.0
Linear(in_features=384, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.1.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (mlp_att_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_att_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_merge_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (mlp_merge_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=384, out_features=128, bias=True)
        )
      )
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (mlp_att_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_att_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_merge_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
      )
    )
    (mlp_merge_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=384, out_features=128, bias=True)
      )
    )
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter
OurGMNCustomInter(
  (mlp_att_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_att_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_merge_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (mlp_merge_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=384, out_features=128, bias=True)
    )
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_att_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_val_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_q.layers
ModuleList(
  (0): Linear(in_features=256, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_q.layers.0
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=384, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_t.layers
ModuleList(
  (0): Linear(in_features=384, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_inter.mlp_merge_t.layers.0
Linear(in_features=384, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.2.gnnm.gmn_intra.gnn_q.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3
GNNWrapper(
  (gnnm): OurGMNCustomWrapper(
    (gmn_inter): OurGMNCustomInter(
      (mlp_att_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_att_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_val_cross_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=128, out_features=128, bias=True)
        )
      )
      (mlp_merge_q): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=256, out_features=128, bias=True)
        )
      )
      (mlp_merge_t): MLP(
        (activation): ELU(alpha=1.0)
        (layers): ModuleList(
          (0): Linear(in_features=384, out_features=128, bias=True)
        )
      )
    )
    (gmn_intra): OurGMNCustomIntra(
      (gnn_t): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
      (gnn_q): GPSLayer(
        summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
        (local_model): GatedGCNLayer()
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (dropout_local): Dropout(p=0.0, inplace=False)
        (dropout_attn): Dropout(p=0.0, inplace=False)
        (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
        (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
        (act_fn_ff): ReLU()
        (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ff_dropout1): Dropout(p=0.0, inplace=False)
        (ff_dropout2): Dropout(p=0.0, inplace=False)
      )
    )
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm
OurGMNCustomWrapper(
  (gmn_inter): OurGMNCustomInter(
    (mlp_att_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_att_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_val_cross_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=128, out_features=128, bias=True)
      )
    )
    (mlp_merge_q): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=256, out_features=128, bias=True)
      )
    )
    (mlp_merge_t): MLP(
      (activation): ELU(alpha=1.0)
      (layers): ModuleList(
        (0): Linear(in_features=384, out_features=128, bias=True)
      )
    )
  )
  (gmn_intra): OurGMNCustomIntra(
    (gnn_t): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
    (gnn_q): GPSLayer(
      summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
      (local_model): GatedGCNLayer()
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (dropout_local): Dropout(p=0.0, inplace=False)
      (dropout_attn): Dropout(p=0.0, inplace=False)
      (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
      (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
      (act_fn_ff): ReLU()
      (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (ff_dropout1): Dropout(p=0.0, inplace=False)
      (ff_dropout2): Dropout(p=0.0, inplace=False)
    )
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter
OurGMNCustomInter(
  (mlp_att_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_att_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_val_cross_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=128, out_features=128, bias=True)
    )
  )
  (mlp_merge_q): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=256, out_features=128, bias=True)
    )
  )
  (mlp_merge_t): MLP(
    (activation): ELU(alpha=1.0)
    (layers): ModuleList(
      (0): Linear(in_features=384, out_features=128, bias=True)
    )
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_att_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_q.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_q.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=128, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_t.layers
ModuleList(
  (0): Linear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_val_cross_t.layers.0
Linear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_q
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=256, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_q.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_q.layers
ModuleList(
  (0): Linear(in_features=256, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_q.layers.0
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_t
MLP(
  (activation): ELU(alpha=1.0)
  (layers): ModuleList(
    (0): Linear(in_features=384, out_features=128, bias=True)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_t.activation
ELU(alpha=1.0)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_t.layers
ModuleList(
  (0): Linear(in_features=384, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_inter.mlp_merge_t.layers.0
Linear(in_features=384, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra
OurGMNCustomIntra(
  (gnn_t): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
  (gnn_q): GPSLayer(
    summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
    (local_model): GatedGCNLayer()
    (self_attn): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
    )
    (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (dropout_local): Dropout(p=0.0, inplace=False)
    (dropout_attn): Dropout(p=0.0, inplace=False)
    (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
    (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
    (act_fn_ff): ReLU()
    (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (ff_dropout1): Dropout(p=0.0, inplace=False)
    (ff_dropout2): Dropout(p=0.0, inplace=False)
  )
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.dropout_local
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.dropout_attn
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_linear1
Linear(in_features=128, out_features=256, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_linear2
Linear(in_features=256, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.act_fn_ff
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.norm2
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_dropout1
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_t.ff_dropout2
Dropout(p=0.0, inplace=False)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q
GPSLayer(
  summary: dim_h=128, local_gnn_type=CustomGatedGCN, global_model_type=Transformer, heads=4
  (local_model): GatedGCNLayer()
  (self_attn): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
  (norm1_local): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (norm1_attn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dropout_local): Dropout(p=0.0, inplace=False)
  (dropout_attn): Dropout(p=0.0, inplace=False)
  (ff_linear1): Linear(in_features=128, out_features=256, bias=True)
  (ff_linear2): Linear(in_features=256, out_features=128, bias=True)
  (act_fn_ff): ReLU()
  (norm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (ff_dropout1): Dropout(p=0.0, inplace=False)
  (ff_dropout2): Dropout(p=0.0, inplace=False)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model
GatedGCNLayer()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.aggr_module
SumAggregation()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.A
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.B
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.C
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.D
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.E
Linear(128, 128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.bn_node_x
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.bn_edge_e
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.act_fn_x
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.local_model.act_fn_e
ReLU()

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.self_attn
MultiheadAttention(
  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.self_attn.out_proj
NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.norm1_local
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.norm1_attn
BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)

dvn.encoder.gnn_wrapper_li.3.gnnm.gmn_intra.gnn_q.dropout_local
Dropout(p=0.0, inplace=False)

